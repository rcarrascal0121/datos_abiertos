# -*- coding: utf-8 -*-
"""Ponderacion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GJqBo42jG5dk38wAm2tYOn04Aj6ubt0C
"""

#importamos librerias para montra drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re

#definicion de path
documento="/content/drive/MyDrive/Colab Notebooks/Dataset/datos.csv"
dataset=pd.read_csv(documento,sep=";")

dataset.head()

"""#Definimos los atributos que influyen en el riesgo

Tomando tus columnas, podemos elegir los atributos más relevantes:

* Gravedad del accidente	Categórico	Heridos/Muertos naturalmente aumentan el riesgo.    
*   Clase de accidente	Categórico	Atropello > Caída > Choque.
*   Frecuencia en ese punto	Numérico	Más eventos = mayor riesgo.
* Hora / Jornada	Categórico	Algunas horas tienen mayor probabilidad de accidentes.
*   Hora Pico
*   Día (laboral vs fin de semana)	Categórico	Cambia la exposición.

# Asignamos pesos a cada atributo (inventados pero coherentes)

Los pesos deben sumar 14, Distribuidos de la siguiente manera:

*   Gravedad del accidente	3.0
*   Clase de accidente	2.0
*   Frecuencia	3.0
*   Hora/Jornada	1.0
*   Hora Pico	3.0
*   Día de la semana	2.0

TOTAL	14

# Definimos valores numéricos para cada categoría
#### A. Gravedad del accidente

* Muertos	-> 3
* Heridos	-> 2
* Material -> 1

#### B. Clase de accidente

* Atropello	-> 3
* Caída ocupante	-> 2
* Choque	-> 1

#### C. Frecuencia

Transformamos la frecuencia a una escala de 1 a 3:

* Frecuencia ≥3 -> 3
* Frecuencia 2 -> 2
* Frecuencia 1 -> 1

#### D. Jornada

* Noche	->3
* Tarde	-> 2
* Mañana	->1

#### HORA PICO 1 O 2

* si -> 2
* no -> 0

(O puedes sumarlos si ambos aparecen)

#### F. Día de la semana

* Vie / Sab / Dom	-> 2
* Lun – Jue	-> 1

# Fórmula de la ponderación final (escala 1–14)

La fórmula general sería:

RIESGO =  (Valor_Gravedad * 3.0/3)+(Valor_Clase * 2/3) + (Valor_Frecuencia * 3/3)  +(Valor_Jornada * 1.0/3) +(Hora_Pico * 3/2) +(Valor_Dia * 2/2)

Cada parte se normaliza a su rango.

El resultado final estará entre 1 y 14.

# Clasificación final por niveles

Rango	Clasificación
* Mayor que 0 y menor igual que 6	Riesgo BAJO
* Mayor que 6 y menor igual que 10	Riesgo MODERADO
* Mayor que 10	Riesgo ALTO
"""

print(dataset.columns.tolist())

fila=0
ponderacion=[]
riesgo=[]
print(len(dataset))

gravedad=0
clase=0
frecuencia=0
jornada=0
hpico=0
dia=0

while fila < len(dataset):

  #Determinacion de la Gravedad del Accidente
  if dataset.iloc[fila]['GRAVEDAD_ACCIDENTE']=="Muertos":
    gravedad=3
  elif dataset.iloc[fila]['GRAVEDAD_ACCIDENTE']=="Heridos":
    gravedad=2
  else:
    gravedad=1

  #Determinacion de la Clase del Accidente
  if dataset.iloc[fila]['CLASE_ACCIDENTE']=="Atropello":
    clase=3
  elif dataset.iloc[fila]['CLASE_ACCIDENTE']=="Caida Ocupante":
    clase=2
  else:
    clase=1

  #Determinacion de la Frecuencia del Accidente
  if dataset.iloc[fila]['FRECUENCIA']==1:
    frecuencia=1
  elif dataset.iloc[fila]['FRECUENCIA']==2:
    frecuencia=2
  else:
    frecuencia=3

  #Determinacion de la jornada
  if dataset.iloc[fila]['Jornada']=="Morning":
    jornada=1
  elif dataset.iloc[fila]['Jornada']=="Tarde":
    jornada=2
  else:
    jornada=3

  #Determinacion de Hora Pico
  if dataset.iloc[fila]['HoraP1']=="Si" and dataset.iloc[fila]['HoraP2']=="Si":
    hpico=2
  elif dataset.iloc[fila]['HoraP1']=="Si" and dataset.iloc[fila]['HoraP2']=="No":
    hpico=2
  elif dataset.iloc[fila]['HoraP1']=="No" and dataset.iloc[fila]['HoraP2']=="Si":
    hpico=2
  else:
    hpico=0

  #Determinacion VIERNES SABADO O DOMINGO
  if dataset.iloc[fila]['DIA']=="Friday" or dataset.iloc[fila]['DIA']=="Saturday" or dataset.iloc[fila]['DIA']=="Sunday":
    dia=2
  else:
    dia=1

  calculo=round(((gravedad*3)/3)+((clase*2)/3)+((frecuencia*3/3)+(jornada*1)/3)+((hpico*3)/2)+((dia*2)/2),2)

  ponderacion.append(calculo)
  if calculo > 0 and calculo <=6:
    riesgo.append("Bajo")
  elif calculo > 6 and calculo <=10:
    riesgo.append("Moderado")
  else:
    riesgo.append("Alto")

  fila=fila+1

dataset['Ponderacion']=ponderacion
dataset['Riesgo']=riesgo

print(dataset.head())

#funcion para normalizar direccion

def normalizar_direccion(text):
    if pd.isna(text):
        return None

    text = text.upper().strip()

    # Corrección de errores de escritura
    reemplazos_comunes = {
        r"\bAVEDNIDA\b": "AVENIDA",
        r"\bCLLE\b": "CALLE",
    }
    for patron, reemplazo in reemplazos_comunes.items():
        text = re.sub(patron, reemplazo, text)

    # Normalización de tipos de vía
    normalizaciones = {
        r"\bAVENIDA\b": "AV",
        r"\bAVE\b": "AV",
        r"\bA CIRCUNVALAR\b": "AV CIRCUNVALAR",
        r"\bAV CIRCUNVALAR\b": "AV CIRCUNVALAR",
        r"\bAV CICRUNVALAR\b": "AV CIRCUNVALAR",
        r"\bAV CIRCUNAVALAR\b": "AV CIRCUNVALAR",

        r"\bCALLE\b": "CL",
        r"\bCL\b": "CL",

        r"\bCARRERA\b": "CR",
        r"\bCRA\b": "CR",
        r"\bKR\b": "CR",
        r"\bKRA\b": "CR",

        r"\bCON\b": " ",  # elimina CON
        r"\bY\b": " ",    # elimina Y
        r"#": " ",        # reemplaza #
        r"-": " ",        # reemplaza guiones
        r"\.": " ",       # reemplaza puntos
    }

    for patron, reemplazo in normalizaciones.items():
        text = re.sub(patron, reemplazo, text)

    # Eliminar dobles espacios
    text = re.sub(r"\s+", " ", text).strip()

    return text

# ---- Procesar archivo completo ----
dfcopia = dataset
dataset["direccion_normalizada"] = dfcopia["direccion"].apply(normalizar_direccion)
dfcopia.to_csv("direcciones_normalizadas.csv", index=False)
print("Proceso completado. Archivo generado: direcciones_normalizadas.csv")

dfcopia.head()

# extraccion de vias principales y demas informacion
import pandas as pd
import re

def parse_direccion(dir):
    if pd.isna(dir):
        return pd.Series([None, None, None, None])

    dir = dir.upper()

    # Normalizar tipos básicos
    dir = (
        dir.replace("CARRERA", "CR")
           .replace("CALLE", "CL")
           .replace("AVENIDA", "AV")
           .replace("AV.", "AV")
           .replace("VA", "VIA")
           .replace("CARERRA", "CR")
           .replace("CIRCUNVALAR", "AV CIRCUNVALAR")
           .replace("K", "CR")

    )

    # 1️⃣ Buscar vías tipo TIPO + número (AV 110, CL 35, CR 53)
    matches = re.findall(r"\b(AV|CL|CR)\s+(\d+[A-Z]?)\b", dir)

    if len(matches) > 0:
        tipo1, num1 = matches[0]
        tipo2, num2 = matches[1] if len(matches) > 1 else (None, None)
        return pd.Series([tipo1, num1, tipo2, num2])

    # 2️⃣ Buscar vías con nombre propio (CORDIALIDAD, CIRCUNVALAR, VIA 40, DIAGONAL, etc.)
    vias_nombre = [
        "CORDIALIDAD",
        "CIRCUNVALAR",
        "VIA 40",
        "VÍA 40",
        "AV CIRCUNVALAR",
        "AV CORDIALIDAD",
        "AVE DEL RIO",
        "DIAGONAL",
        "DIAG",
        "TRONCAL",
        "AUTOPISTA",
        "CARRETERA",
        "AV MURILLO",
        "AV CIUDAD CARIBE",
        "AV DEL RIO",
        "KILOMETRO",
        "PUENTE PUMAREJO",
        "TRANSV",
        "VIA JUAN MINA",
        "VIA LA PLAYA"
        "CORREDOR PORTUARIO",
        "GALAPA",
        "VIA BARRANQUILLA",
        "VIA GALAPA",
        "VIA LA PROSPERIDAD"
    ]

    for via in vias_nombre:
        if via in dir:
            return pd.Series(["NOMBRE", via, None, None])

    # Si no encontró nada
    return pd.Series([None, None, None, None])

dfcopia[["tipo_principal", "num_principal", "tipo_secundaria", "num_secundaria"]] = (dfcopia["direccion_normalizada"].apply(parse_direccion))

dfcopia.head()

dfcopia = dfcopia.drop(columns=["direccion"])

# Guardar con codificación UTF-8 y delimitador estándar
dataset.to_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/DATASET_PROCESADO_Prueba.csv',
               index=False,
               encoding='utf-8')
print("✅ Dataset guardado como CSV UTF-8")